{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from transformers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tokenizer loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/hugging_face/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "max_len = tokenizer.max_len\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2182,  2003,  2070,  3793,  2000,  4372, 16044,   102]])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0549,  0.1053, -0.1065,  ..., -0.3550,  0.0686,  0.6506],\n",
       "         [-0.5759, -0.3650, -0.1383,  ..., -0.6782,  0.2092, -0.1639],\n",
       "         [-0.1641, -0.5597,  0.0150,  ..., -0.1603, -0.1346,  0.6216],\n",
       "         ...,\n",
       "         [ 0.2448,  0.1254,  0.1587,  ..., -0.2749, -0.1163,  0.8809],\n",
       "         [ 0.0481,  0.4950, -0.2827,  ..., -0.6097, -0.1212,  0.2527],\n",
       "         [ 0.9046,  0.2137, -0.5897,  ...,  0.3040, -0.6172, -0.1950]]])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date']"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('float')\n",
    "tokenizer.tokenize('total')\n",
    "tokenizer.tokenize('gst')\n",
    "tokenizer.tokenize('round')\n",
    "tokenizer.tokenize('10%') \n",
    "tokenizer.tokenize('9.24')\n",
    "tokenizer.tokenize('273500') \n",
    "tokenizer.tokenize('currency')\n",
    "tokenizer.tokenize('row')\n",
    "tokenizer.tokenize('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load('/home/long8v/ICDAR-2019-SROIE/task3/data/data_dict4.pth')\n",
    "zipped_data = list(zip(*data_dict.values()))\n",
    "texts = zipped_data[0]\n",
    "labels = zipped_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = list(map(lambda e: e.replace('\\n', ' row '), texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TAN WOON YANN\\nBOOK TA .K(TAMAN DAYA) SDN BND\\n789417-W\\nNO.53 55,57 & 59, JALAN SAGU 18,\\nTAMAN DAYA,\\n81100 JOHOR BAHRU,\\nJOHOR.\\nDOCUMENT NO : TD01167104\\nDATE:\\t25/12/2018 8:13:39 PM\\nCASHIER:\\tMANIS\\nMEMBER:\\nCASH BILL\\nCODE/DESC\\tPRICE\\tDISC\\tAMOUNT\\nQTY\\tRM\\tRM\\n9556939040116\\tKF MODELLING CLAY KIDDY FISH\\n1 PC\\t*\\t9.000\\t0.00\\t9.00\\nTOTAL:\\t9.00\\nROUNDING ADJUSTMENT:\\t0.00\\nROUNDED TOTAL (RM):\\t9.00\\nCASH\\t10.00\\nCHANGE\\t1.00\\nGOODS SOLD ARE NOT RETURNABLE OR\\nEXCHANGEABLE\\n***\\n***\\nTHANK YOU\\nPLEASE COME AGAIN !',\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4,\n",
       "        4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int32))"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(texts[0], labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace special token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_int = re.compile('\\d+')\n",
    "re_float = re.compile('(\\d+\\.\\d+)')\n",
    "re_percent = re.compile('(\\d+.?\\d+%)')\n",
    "re_date = re.compile('(\\d{2}[/-]\\d{2}[/-]\\d{2,4})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20-02-2020']"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_date.findall('20-02-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dict = {re_float:'float', re_percent:'percent', re_date:'date', re_int:'int'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float : 0\n",
      "percent : 0\n",
      "date : 0\n",
      "int : 0\n"
     ]
    }
   ],
   "source": [
    "corpus = ' '.join(texts)\n",
    "for _ in re_dict.values():\n",
    "    print('{} : {}'.format(_, sum([_ in corpus])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27'"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_int.search(texts[1]).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in zip(texts, labels):\n",
    "    if len(text)!=len(label):\n",
    "        print(len(text), len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_token(text, label):\n",
    "    text_copy = text\n",
    "    label = list(label)\n",
    "    label_copy = label[:]\n",
    "    for re_exp, special_token in re_dict.items():\n",
    "        while re_exp.search(text):\n",
    "            match = re_exp.search(text)\n",
    "            span_start, span_end = match.span()\n",
    "            word = match.group()\n",
    "            len_diff = len(special_token) - len(word)\n",
    "            if len_diff > 0:\n",
    "                while len_diff:\n",
    "                    len_diff -= 1\n",
    "                    try:\n",
    "                        label_copy.insert(span_start, label_copy[span_start])\n",
    "                    except:\n",
    "                        print(len(label), span_start)\n",
    "            elif len_diff < 0: \n",
    "                del label_copy[span_start:span_start-len_diff]\n",
    "            else:\n",
    "                pass\n",
    "            text = list(text)\n",
    "            del text[span_start:span_end]\n",
    "            text.insert(span_start, special_token)\n",
    "            text = ''.join(text)\n",
    "    assert len(text) == len(label_copy)\n",
    "    return text, label_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_list = dict([replace_special_token(text, label) for text, label in zip(texts, labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(replace_list.keys())\n",
    "labels = list(replace_list.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bio-tagging for bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_word(text):\n",
    "    token_word = tokenizer.tokenize(text)\n",
    "    return token_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_token_labels(token_word, text, label):\n",
    "    index = 0\n",
    "    token_labels = []\n",
    "    label_clean = [lbl for txt, lbl in list(zip(text, label)) if txt.strip()]\n",
    "    for token in token_word[:max_len]:\n",
    "        token_clean = token.replace('##', '')\n",
    "        token_labels.append(label_clean[index:index+len(token_clean)])\n",
    "        index += len(token_clean)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_tag(token_labels):\n",
    "    label_dict = {0: 'O', 1: 'COMPANY', 2:'DATE', 3:'ADDRESS', 4:'TOTAL'}\n",
    "    token_label_bio = []\n",
    "    current = 0 \n",
    "    for token_label in token_labels:\n",
    "        try:\n",
    "            temp_label = token_label[0]\n",
    "        except IndexError as e:\n",
    "            pass\n",
    "        if temp_label == 0:\n",
    "            token_label_bio.append(label_dict[temp_label])\n",
    "        elif temp_label != current:\n",
    "            token_label_bio.append('B-{}'.format(label_dict[temp_label]))\n",
    "        else:\n",
    "            token_label_bio.append('I-{}'.format(label_dict[temp_label]))\n",
    "        current = temp_label\n",
    "    return token_label_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_token(text, label):\n",
    "    token_word = get_tokenized_word(text)\n",
    "    token_labels = get_token_labels(token_word, text, label)\n",
    "    token_label_bio = get_bio_tag(token_labels)\n",
    "    return pd.DataFrame(zip(token_word, token_label_bio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_token_text_label(texts, labels):\n",
    "    df_list = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        df = pd.DataFrame()\n",
    "        df = df.append({0:'-DOCSTART-', 1: 'O'}, ignore_index=True)\n",
    "        df = df.append(get_paired_token(text, label))\n",
    "        df = df.append({0:'', 1:'O'}, ignore_index=True)\n",
    "        df[0] = df[0].apply(lambda e: replace_digits(e))\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_text, test_text,  train_label, test_label = train_test_split(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, val_text,  train_label, val_label = train_test_split(train_text, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = get_paired_token_text_label(train_text, train_label)\n",
    "val_df = get_paired_token_text_label(val_text, val_label)\n",
    "test_df = get_paired_token_text_label(test_text, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('data/train_df.p', 'wb') as f:\n",
    "    pickle.dump(train_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_long = reduce(lambda a, b: pd.concat([a,b]), train_df)\n",
    "val_df_long = reduce(lambda a, b: pd.concat([a,b]), val_df)\n",
    "test_df_long = reduce(lambda a, b: pd.concat([a,b]), test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_long.to_csv('data/train.txt', sep=' ', index=False, header=False)\n",
    "val_df_long.to_csv('data/valid.txt', sep=' ', index=False, header=False)\n",
    "test_df_long.to_csv('data/test.txt', sep=' ', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_hugging_face",
   "language": "python",
   "name": "huggin_face"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
