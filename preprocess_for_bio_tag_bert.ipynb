{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tokenizer loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/hugging_face/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "max_len = tokenizer.max_len\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('â€»')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2182,  2003,  2070,  3793,  2000,  4372, 16044,   102]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0549,  0.1053, -0.1065,  ..., -0.3550,  0.0686,  0.6506],\n",
       "         [-0.5759, -0.3650, -0.1383,  ..., -0.6782,  0.2092, -0.1639],\n",
       "         [-0.1641, -0.5597,  0.0150,  ..., -0.1603, -0.1346,  0.6216],\n",
       "         ...,\n",
       "         [ 0.2448,  0.1254,  0.1587,  ..., -0.2749, -0.1163,  0.8809],\n",
       "         [ 0.0481,  0.4950, -0.2827,  ..., -0.6097, -0.1212,  0.2527],\n",
       "         [ 0.9046,  0.2137, -0.5897,  ...,  0.3040, -0.6172, -0.1950]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bio-tagging for bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_word(text):\n",
    "    token_word = tokenizer.tokenize(text)\n",
    "    return token_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_token_labels(token_word, text, label):\n",
    "    index = 0\n",
    "    token_labels = []\n",
    "    label_clean = [lbl for txt, lbl in list(zip_longest(text, label)) if txt.strip()]\n",
    "    for token in token_word[:max_len]:\n",
    "        token_clean = token.replace('##', '')\n",
    "        token_labels.append(label_clean[index:index+len(token_clean)])\n",
    "        index += len(token_clean)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_tag(token_labels):\n",
    "    label_dict = {0: 'O', 1: 'COMPANY', 2:'DATE', 3:'ADDRESS', 4:'TOTAL'}\n",
    "    token_label_bio = []\n",
    "    current = 0 \n",
    "    for token_label in token_labels:\n",
    "        try:\n",
    "            temp_label = token_label[0]\n",
    "        except IndexError as e:\n",
    "            pass\n",
    "        if temp_label == 0:\n",
    "            token_label_bio.append(label_dict[temp_label])\n",
    "        elif temp_label != current:\n",
    "            token_label_bio.append('B-{}'.format(label_dict[temp_label]))\n",
    "        else:\n",
    "            token_label_bio.append('I-{}'.format(label_dict[temp_label]))\n",
    "        current = temp_label\n",
    "    return token_label_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_token(text, label):\n",
    "    token_word = get_tokenized_word(text)\n",
    "    token_labels = get_token_labels(token_word, text, label)\n",
    "    token_label_bio = get_bio_tag(token_labels)\n",
    "    return pd.DataFrame(zip_longest(token_word, token_label_bio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_token_text_label(texts, labels):\n",
    "    df_list = []\n",
    "    for text, label in zip_longest(texts, labels):\n",
    "        df = pd.DataFrame()\n",
    "        df = df.append({0:'-DOCSTART-', 1: 'O'}, ignore_index=True)\n",
    "        df = df.append(get_paired_token(text, label))\n",
    "        df = df.append({0:'', 1:'O'}, ignore_index=True)\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load('/home/long8v/ICDAR-2019-SROIE/task3/data/data_dict4.pth')\n",
    "zipped_data = list(zip_longest(*data_dict.values()))\n",
    "texts = zipped_data[0]\n",
    "labels = zipped_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text,  train_label, test_label = train_test_split(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, val_text,  train_label, val_label = train_test_split(train_text, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = get_paired_token_text_label(train_text, train_label)\n",
    "val_df = get_paired_token_text_label(val_text, val_label)\n",
    "test_df = get_paired_token_text_label(test_text, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_long = reduce(lambda a, b: pd.concat([a,b]), train_df)\n",
    "val_df_long = reduce(lambda a, b: pd.concat([a,b]), val_df)\n",
    "test_df_long = reduce(lambda a, b: pd.concat([a,b]), test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_long.to_csv('data/train.txt', sep=' ', index=False, header=False)\n",
    "val_df_long.to_csv('data/valid.txt', sep=' ', index=False, header=False)\n",
    "test_df_long.to_csv('data/test.txt', sep=' ', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_hugging_face",
   "language": "python",
   "name": "huggin_face"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
