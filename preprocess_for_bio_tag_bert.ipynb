{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tokenizer loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_len = tokenizer.max_len\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('※')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2182,  2003,  2070,  3793,  2000,  4372, 16044,   102]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0549,  0.1053, -0.1065,  ..., -0.3550,  0.0686,  0.6506],\n",
       "         [-0.5759, -0.3650, -0.1383,  ..., -0.6782,  0.2092, -0.1639],\n",
       "         [-0.1641, -0.5597,  0.0150,  ..., -0.1603, -0.1346,  0.6216],\n",
       "         ...,\n",
       "         [ 0.2448,  0.1254,  0.1587,  ..., -0.2749, -0.1163,  0.8809],\n",
       "         [ 0.0481,  0.4950, -0.2827,  ..., -0.6097, -0.1212,  0.2527],\n",
       "         [ 0.9046,  0.2137, -0.5897,  ...,  0.3040, -0.6172, -0.1950]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['currency']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('float')\n",
    "tokenizer.tokenize('total')\n",
    "tokenizer.tokenize('gst')\n",
    "tokenizer.tokenize('round')\n",
    "tokenizer.tokenize('10%') # ['10', '%']\n",
    "tokenizer.tokenize('9.24') # ['9', '.', '24']\n",
    "tokenizer.tokenize('273500') # ['273', '##500']\n",
    "tokenizer.tokenize('currency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load('/home/long8v/ICDAR-2019-SROIE/task3/data/data_dict4.pth')\n",
    "zipped_data = list(zip(*data_dict.values()))\n",
    "texts = zipped_data[0]\n",
    "labels = zipped_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace float numbers to 'float' token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_digit = re.compile('\\b(\\d+)\\b')\n",
    "re_int = re.compile('(\\d+)')\n",
    "# re_currency = re.compile('\\d{1,3}(\\,\\d{3})*|(\\d+))(\\.\\d{2})?')\n",
    "re_float = re.compile('(\\d+\\.\\d+)')\n",
    "re_percent = re.compile('(\\d+.?\\d+%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dict = {re_digit:'int', re_float:'float', re_percent:'percent', re_int:'int'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_digits(text):\n",
    "    for key, value in re_dict.items():\n",
    "        text = key.sub(value, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는 삼성전자를 int원에 먹었는데 percent를 현재는 float원을 가지고 있다'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_digits('나는 삼성전자를 500원에 먹었는데 10%를 현재는 550.0원을 가지고 있다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int : 0\n",
      "float : 0\n",
      "percent : 0\n",
      "int : 0\n"
     ]
    }
   ],
   "source": [
    "corpus = ' '.join(texts)\n",
    "for _ in re_dict.values():\n",
    "    print('{} : {}'.format(_, sum([_ in corpus])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(map(lambda e: replace_digits(e), texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bio-tagging for bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_word(text):\n",
    "    token_word = tokenizer.tokenize(text)\n",
    "    return token_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_token_labels(token_word, text, label):\n",
    "    index = 0\n",
    "    token_labels = []\n",
    "    label_clean = [lbl for txt, lbl in list(zip(text, label)) if txt.strip()]\n",
    "    for token in token_word[:max_len]:\n",
    "        token_clean = token.replace('##', '')\n",
    "        token_labels.append(label_clean[index:index+len(token_clean)])\n",
    "        index += len(token_clean)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_tag(token_labels):\n",
    "    label_dict = {0: 'O', 1: 'COMPANY', 2:'DATE', 3:'ADDRESS', 4:'TOTAL'}\n",
    "    token_label_bio = []\n",
    "    current = 0 \n",
    "    for token_label in token_labels:\n",
    "        try:\n",
    "            temp_label = token_label[0]\n",
    "        except IndexError as e:\n",
    "            pass\n",
    "        if temp_label == 0:\n",
    "            token_label_bio.append(label_dict[temp_label])\n",
    "        elif temp_label != current:\n",
    "            token_label_bio.append('B-{}'.format(label_dict[temp_label]))\n",
    "        else:\n",
    "            token_label_bio.append('I-{}'.format(label_dict[temp_label]))\n",
    "        current = temp_label\n",
    "    return token_label_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_token(text, label):\n",
    "    token_word = get_tokenized_word(text)\n",
    "    token_labels = get_token_labels(token_word, text, label)\n",
    "    token_label_bio = get_bio_tag(token_labels)\n",
    "    return pd.DataFrame(zip(token_word, token_label_bio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_token_text_label(texts, labels):\n",
    "    df_list = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        df = pd.DataFrame()\n",
    "        df = df.append({0:'-DOCSTART-', 1: 'O'}, ignore_index=True)\n",
    "        df = df.append(get_paired_token(text, label))\n",
    "        df = df.append({0:'', 1:'O'}, ignore_index=True)\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text,  train_label, test_label = train_test_split(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, val_text,  train_label, val_label = train_test_split(train_text, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = get_paired_token_text_label(train_text, train_label)\n",
    "val_df = get_paired_token_text_label(val_text, val_label)\n",
    "test_df = get_paired_token_text_label(test_text, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_long = reduce(lambda a, b: pd.concat([a,b]), train_df)\n",
    "val_df_long = reduce(lambda a, b: pd.concat([a,b]), val_df)\n",
    "test_df_long = reduce(lambda a, b: pd.concat([a,b]), test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_long.to_csv('data/train.txt', sep=' ', index=False, header=False)\n",
    "val_df_long.to_csv('data/valid.txt', sep=' ', index=False, header=False)\n",
    "test_df_long.to_csv('data/test.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_hugging_face",
   "language": "python",
   "name": "huggin_face"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
