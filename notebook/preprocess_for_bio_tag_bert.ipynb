{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from transformers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tokenizer loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_len = tokenizer.max_len\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('flo')\n",
    "# tokenizer.tokenize('total')\n",
    "# tokenizer.tokenize('gst')\n",
    "# tokenizer.tokenize('round')\n",
    "# tokenizer.tokenize('10%') \n",
    "# tokenizer.tokenize('9.24')\n",
    "# tokenizer.tokenize('273500') \n",
    "# tokenizer.tokenize('currency')\n",
    "tokenizer.tokenize('row')\n",
    "tokenizer.tokenize('date')\n",
    "tokenizer.tokenize('percent')\n",
    "tokenizer.tokenize('in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load('/home/long8v/ICDAR-2019-SROIE/task3/data/data_dict4.pth')\n",
    "zipped_data = list(zip(*data_dict.values()))\n",
    "texts = zipped_data[0]\n",
    "labels = zipped_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace special token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_int = re.compile('\\d+')\n",
    "re_float = re.compile('(\\d+\\.\\d+)')\n",
    "re_percent = re.compile('(\\d+.?\\d+%)')\n",
    "re_date = re.compile('\\d{2}[/-]\\d{2}[/-]\\d{2,4}')\n",
    "re_date2 = re.compile('\\d{2,4}\\s?(?:JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)\\s?\\d{2,4}')\n",
    "re_row = re.compile('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dict = {re_float:' float ', re_percent:' percent ', re_date:' date ', \n",
    "           re_date2: ' date ', re_int:' int ', re_row:' row '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " float  : 0\n",
      " percent  : 0\n",
      " date  : 0\n",
      " date  : 0\n",
      " int  : 0\n",
      " row  : 0\n"
     ]
    }
   ],
   "source": [
    "corpus = ' '.join(texts)\n",
    "for _ in re_dict.values():\n",
    "    print('{} : {}'.format(_, sum([_ in corpus])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in zip(texts, labels):\n",
    "    if len(text)!=len(label):\n",
    "        print(len(text), len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_digits(text):\n",
    "    for key, value in re_dict.items():\n",
    "        text = key.sub(value, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_token(text, label):\n",
    "    text_copy = text\n",
    "    label = list(label)\n",
    "    label_copy = label[:]\n",
    "    for re_exp, special_token in re_dict.items():\n",
    "        while re_exp.search(text):\n",
    "            match = re_exp.search(text)\n",
    "            span_start, span_end = match.span()\n",
    "            word = match.group()\n",
    "            len_diff = len(special_token) - len(word)\n",
    "            if len_diff > 0:\n",
    "                while len_diff:\n",
    "                    len_diff -= 1\n",
    "                    try:\n",
    "                        label_copy.insert(span_start, label_copy[span_start])\n",
    "                    except:\n",
    "                        print(len(label), span_start)\n",
    "            elif len_diff < 0: \n",
    "                del label_copy[span_start:span_start-len_diff]\n",
    "            else:\n",
    "                pass\n",
    "            text = list(text)\n",
    "            del text[span_start:span_end]\n",
    "            text.insert(span_start, special_token)\n",
    "            text = ''.join(text)\n",
    "    assert len(text) == len(label_copy)\n",
    "    return text, label_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_list = dict([replace_special_token(text, label) \n",
    "                     for text, label in zip(texts, labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(replace_list.keys())\n",
    "labels = list(replace_list.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bio-tagging for bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_word(text):\n",
    "    token_word = tokenizer.tokenize(text)\n",
    "    return token_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_token_labels(token_word, text, label):\n",
    "    index = 0\n",
    "    token_labels = []\n",
    "    label_clean = [lbl for txt, lbl in list(zip(text, label)) if txt.strip()]\n",
    "    for token in token_word[:max_len]:\n",
    "        token_clean = token.replace('##', '')\n",
    "        token_labels.append(label_clean[index:index+len(token_clean)])\n",
    "        index += len(token_clean)\n",
    "    return token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_tag(token_labels):\n",
    "    label_dict = {0: 'O', 1: 'COMPANY', 2:'DATE', 3:'ADDRESS', 4:'TOTAL'}\n",
    "    token_label_bio = []\n",
    "    current = 0 \n",
    "    temp_label = 0\n",
    "    for token_label in token_labels:\n",
    "        try:\n",
    "            temp_label = token_label[0]\n",
    "        except IndexError as e:\n",
    "            pass\n",
    "        if temp_label == 0:\n",
    "            token_label_bio.append(label_dict[temp_label])\n",
    "        elif temp_label != current:\n",
    "            token_label_bio.append('B-{}'.format(label_dict[temp_label]))\n",
    "        else:\n",
    "            token_label_bio.append('I-{}'.format(label_dict[temp_label]))\n",
    "        current = temp_label\n",
    "    return token_label_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_token(text, label):\n",
    "    token_word = get_tokenized_word(text)\n",
    "    token_labels = get_token_labels(token_word, text, label)\n",
    "    token_label_bio = get_bio_tag(token_labels)\n",
    "    return pd.DataFrame(zip(token_word, token_label_bio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_token_text_label(texts, labels):\n",
    "    df_list = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        df = pd.DataFrame()\n",
    "        df = df.append({0:'-DOCSTART-', 1: 'O'}, ignore_index=True)\n",
    "        df = df.append(get_paired_token(text, label))\n",
    "        df = df.append({0:'', 1:'O'}, ignore_index=True)\n",
    "        df[0] = df[0].apply(lambda e: replace_digits(e))\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_text, test_text,  train_label, test_label = train_test_split(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, val_text,  train_label, val_label = train_test_split(train_text, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = get_paired_token_text_label(train_text, train_label)\n",
    "val_df = get_paired_token_text_label(val_text, val_label)\n",
    "test_df = get_paired_token_text_label(test_text, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_long = reduce(lambda a, b: pd.concat([a,b]), train_df)\n",
    "val_df_long = reduce(lambda a, b: pd.concat([a,b]), val_df)\n",
    "test_df_long = reduce(lambda a, b: pd.concat([a,b]), test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted([df.shape for df in train_df], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_long.to_csv('data/train.txt', sep=' ', index=False, header=False)\n",
    "val_df_long.to_csv('data/valid.txt', sep=' ', index=False, header=False)\n",
    "test_df_long.to_csv('data/test.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_hugging_face",
   "language": "python",
   "name": "huggin_face"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
